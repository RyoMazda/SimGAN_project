# Application of SimGAN (by Apple's paper) for Hand Writing Recognition

## Outline
### What Original Paper does

[Apple's paper (SimGAN = Simulated + Unsupervised Learning GAN) Link](https://arxiv.org/abs/1612.07828)

People want to improve the accuracy of eye direction recognition.
The problem there was it is difficult to prepare traing data.
Although it is easy to get images of peoples's eye, it is costly to put labels (which are answers for training, eye direction vector in this case).
In order to overcome this not-enough-training-data problem, some guys used Unity Eye to generate eye images with labels.
(As you may know Unity is a famous game engine, and Unity Eye is, I guess, kind of package to generate 3D data of eyes in Unity.
Of course we can control the eye direction, which means these can be used as traing data for eye direction recognition.)
This led to the state-of-the-art result at that time.

However, these images are fake, not real images.
Therefore, guys from Apple tried to "refine" the fake images (with labels) combining them with real images (without labels),
and the paper is about how they did it and how was the result.

The schematic figure of the algorithm is below (taken from the original paper):

![SimGAN for eyes direction recognition](/images/images_for_README/simgan.png)

* Simulator is Unity Eye
* Synthetic images are fake 3D CG images of an eye (with labels = information of eye direction) generated by Unity Eye
* Unlabeled real images are real images, which we can get from anywhere but are unrealistic to put labels on millions of them.
* Refiner generates Refined images out of fake images. The definition of Refined is
  1. it looks more like real images
  2. it preserves the label (direction of eye)
* Discriminator is trained to discriminate real images and fake images, while Refiner decieve the Discriminator
* The distance between original fake image and refined image is added to loss function of Refiner so that it preserves the labels.
* Refined images are used as traing data for eye direction recognition


### Ideal Goal

People want to improve the accuracy of hand writing recognition.
To that end, we need as much training data as possible with as much diversity as possible.
While there are many hand writing images available, it is costly to put labels (character codes) on millions of them.
Let's apply the idea we learned above!

In this case,
* Synthetic images are images of a letter with labels (fonts can be used here. hand writing images are okay if they have labels)
* Unlabeled real images are hand writing letters
* Refined images are images of a letter with labels, which looks more like real images

By producing Refined images and using them as traing data, the accuracy of hand writing recognition is expected to be improved.


### What I did to test if the idea works

I constrained myself to numbers (0,1,2,...,9) to test the idea, and this is all I did so far.

I used
* fonts as Synthetic images (original data is [this](/data/font_images), but I resized and normalized it [(this)](/data/processed_fonts)
* MNIST as real images and pretended that they don't have labels [(this)](/data/MNIST_data)
* Refined images should look more like MNIST than fonts

Before this, I implemented GAN with MNIST as a practice of adversarial training.
You can see the description [here](/GAN_Practice.md) if you are interested.

## Details

### Model
#### Hyperparameters (Comparison with the original paper)

coming soon...

### Result1: only eight

I first used only number eight to see how it works.
One of the results is this;

![result](/sample_output/SimGAN/eight_only_best/104.png)

The upper half of the images is original fonts images and the bottom is corresponding refined images.

You can see that thick fonts are transformed into thinner, while thin into thicker.
This is because MNIST data (= real hand writing images) has uniform thickness and refiner learned this feature.

You can see the full sample outputs [here](/sample_output/SimGAN/eight_only_best).

### Result2: all numbers

Next I used all the numbers.

![result](/sample_output/SimGAN/all_numbers/082.png)

The difference is subtle but if you can see carefully you will find that the right edge of the refined images (bottom half) have "small tail" compared with the original ifont mages (upper half).

You can see the full sample outputs [here](/sample_output/SimGAN/all_numbers).

The difference is too subtle so I think I should tune the parameter (for example, I can try smaller differential weight for refined images to absorb more information from real images).

Before that, I performed two tests below (Result3 and Result4) to see if the refined images are really learning features from real images.

### Result3: add horizontal line to real images

I added a horizontal line in the middle of all the MNIST images before the training.

!<img src="/images/images_for_README/horizontal_line_example.png" width="120px">

The result is this.

![result](/sample_output/SimGAN/horizontal/038.png)

As you can see, the refiner seems to respect horizontal lines and look donw on vertical lines.

You can see the full sample outputs [here](/sample_output/SimGAN/horizontal).

### Result4: deform real images

Next I modified the real images drastically like below, by calculating the gradient of the original real images.

!<img src="/images/images_for_README/gradient_example.png" width="120px">

The result:

![result](/sample_output/SimGAN/gradient/132.png)

As you can see, the refiner learned the gradient feature.

You can see the full sample outputs [here](/sample_output/SimGAN/gradient).


## Future Work

* tune hyperparameters so that fonts are refined more into MNIST data
* use refined images as training data for MNIST recognition and see if the accuracy is improved
* try not-CNN architecture to learn local features of real images


## Environment

* Python 3.5.1
* Tensorflow 1.0.1
